{"cells":[{"cell_type":"markdown","source":["# Data Science workflow  \n\nIn this sequence of notebooks, we will exemplify the inner steps in the Data Science workflow.  \nWe are not going to discuss the business requirements and deployment strategies, but just the phases below:\n\n##### I - Exploratory Data Analysis \n### II - Feature Engineering and Selection (this notebook)  \n##### III - Modeling  \n##### IV - Evaluation  \n\nIn this notebook, we will use the insights we have gathered previously in the EDA and perform the preparation for the Modeling phase.  \nWe will perform the following steps:\n1. Data Loading and Cleaning\n2. Feature Selection\n3. Feature Engineering"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6dce646d-c681-45e5-bb7f-599ebc0ffa5b","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## II - Feature Engineering and Selection"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"96f3b935-9f71-4b5f-8ef6-16ad6a9f05ea","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["##### Feature Engineering and Selection is named \"Data Preparation\" in CRISP-DM and \"Data Engineering\" in CRISP-ML:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3b465216-41ae-40c2-8072-5da65dccdeb1","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## CRISP-DM\n\n\n[CRISP-DM Process](https://miro.medium.com/max/736/1*0-mnwXXLlMB_bEQwp-706Q.png)\n\n<br>\n<img src=\"https://miro.medium.com/max/736/1*0-mnwXXLlMB_bEQwp-706Q.png\" width=\"768\" height=\"512\" />\n\n## CRISP-ML\n\n[CRISP-ML Process](https://ml-ops.org/img/crisp-ml-process.jpg)  \n[Source](https://ml-ops.org/content/crisp-ml)\n\n<br>\n<img src=\"https://ml-ops.org/img/crisp-ml-process.jpg\" width=\"1024\" height=\"512\" />"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"23e36b77-67c3-4d25-82a4-1e3d7b65f157","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## 1. Data Loading and Cleaning\nAfter the data has been analyzed, it is time to clean it. In other words, the data cleaning step will involve handling the problems that were encountered with the dataset during analysis. So, data professionals will replace missing values with neighborhood average/minimum/maximum/ any other values in this step. Any other incorrect data points will also be fixed in this step. Data Cleaning also constitutes the removal of outlying data points.\n\n### 1.1 - Import libraries"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1f45c1d2-f87f-4cc7-81c1-ff8179e557b8","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cb328288-f959-4dfe-b996-8230d30a2456","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 1.2 - Load Dataset and check basic info"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7234a78b-b613-484a-a407-e92e5fc0aad5","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["##### Visually inspecting the dataset"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2194ad85-ce09-4d6d-88e7-d07113c96ff6","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df = pd.read_csv('data/Automobile_data.csv')\ndf.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7b4691ef-f56a-46d6-938c-e14eba29714d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### Checking columns and data types"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1e5db9a7-3476-4da3-840f-378419253dab","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#df.columns\ndf.info()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9f8ba5a7-64ee-445d-97f7-cebde1a0574e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### At this moment, you look for columns that shall be transformed/converted later in the workflow."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"32463a59-8e59-4c72-b533-37bd27f544d9","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["print(df.select_dtypes(include='number').columns)\nprint(df.select_dtypes(include='object').columns)\nprint(df.select_dtypes(include='category').columns)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1fac34da-b9e5-4f96-8daa-3afc6ea96d3f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 3. Check for missing values"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5b70dbbf-e32f-4c29-9449-b8e5644e9f38","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.isnull().sum()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"95a72cf4-7602-44e8-99c8-8b52c00ac40e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["It seems there are not missing values, but that may be misleading. Let's explore a bit more:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5c67b6d0-5140-46d3-b495-32552d3b9235","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#Checking for wrong entries like symbols -,?,#,*,etc.\nfor col in df.columns:\n    print('{} : {}'.format(col, df[col].unique()))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"946d7794-a1c2-4af9-8ddd-b2acd4c85030","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["There are null values in our dataset in form of ‘?’ only but Pandas is not recognizing them so we will replace them into np.nan form."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9a79a7c0-b823-42ca-9965-7fd2998983df","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["for col in df.columns:\n    df[col].replace({'?': np.nan},inplace=True)\n    \ndf.info()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"afa71d14-c6f6-4518-9d6f-12bd44e552d3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.isnull().sum()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7cadafee-242a-47ab-ab07-b2389cca3ed6","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 3.1 Visualizing the missing values  \nNow the missing values are identified in the dataframe. With the help of heatmap, we can see the amount of data that is missing from the attribute. With this, we can make decisions whether to drop these missing values or to replace them. Usually dropping the missing values is not advisable but sometimes it may be helpful too."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d5a17714-55d3-4db4-99a3-bbf7363f3fac","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["plt.figure(figsize=(12,10))\nsns.heatmap(df.isnull(),cbar=False,cmap='viridis')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fc3cdd7f-5cec-42e0-a59f-95087f82a7d0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now observe that there are many missing values in normalized_losses while other columns have fewer missing values. We can’t drop the normalized_losses column as it may be important for our prediction.  \nWe can also use the **missingno** libray for a better evaluation of the missing values. First we can check the quantity and how they distribute among the rows:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"199edd31-9dbc-44d9-8684-fa692e3aae26","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import missingno as msno"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cc51a006-0d1f-4cc3-b0c8-e282aff74a80","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["msno.bar(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a93ab6f2-107f-4b88-9cce-53330b47f262","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["msno.matrix(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2a2381d6-86ca-4ca9-af64-4e941378c00d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The missingno correlation heatmap measures nullity correlation: how strongly the presence or absence of one variable affects the presence of another"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"79f64aaa-6dc2-4426-93e7-ed5eab900137","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["msno.heatmap(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1777940c-7120-4e72-a079-569d8d79d5c6","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The dendrogram allows you to more fully correlate variable completion, revealing trends deeper than the pairwise ones visible in the correlation heatmap"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ca8abfb6-20bd-4cc0-a305-44d162218207","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["msno.dendrogram(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4fc45a7a-c877-4ba1-bdbd-c7e1122fb233","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 3.2. Replacing the missing values\nWe will be replacing these missing values with mean because the number of missing values is not great (we could have used the median too).  \nLater, in the data preparation phase, we will learn other imputation techniques."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"195ca973-2746-41e4-ae93-fe23e7ddb589","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.select_dtypes(include='number').head()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"301d5644-5881-444a-b42d-fe3ee20706e9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df.select_dtypes(include='object').head()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fa322fbc-aa52-4f11-b403-c0a58c3681af","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now let's transform the mistaken datatypes for numeric values and fill with the mean, using the strategy we have chosen."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"905ebb14-1eaf-45f3-8591-f87f4bf282f7","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["num_col = ['normalized-losses', 'bore',  'stroke', 'horsepower', 'peak-rpm','price']\nfor col in num_col:\n    df[col] = pd.to_numeric(df[col])\n    df[col].fillna(df[col].mean(), inplace=True)\ndf.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c4a41922-ca4b-4886-b6bb-c115d4c4fbd5","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##### Checking for outliers\n\n![Outliers](https://sphweb.bumc.bu.edu/otlt/MPH-Modules/PH717-QuantCore/PH717-Module6-RandomError/Normal%20Distribution%20deviations.png)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6f4136a4-70cb-4539-82ed-bd0e503b9f2a","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["Techniques for outlier detection and removal:\n👉 Z-score treatment :\n\nAssumption– The features are normally or approximately normally distributed.\n\n\nStep-2: Read and Load the Dataset\n\ndf = pd.read_csv('placement.csv')\ndf.sample(5)\nDetect and remove outliers data cgpa\n\n\nStep-4: Finding the Boundary Values\n\nprint(\"Highest allowed\",df['cgpa'].mean() + 3*df['cgpa'].std())\nprint(\"Lowest allowed\",df['cgpa'].mean() - 3*df['cgpa'].std())\nOutput:\n\nHighest allowed 8.808933625397177\nLowest allowed 5.113546374602842\nStep-5: Finding the Outliers\n\ndf[(df['cgpa'] > 8.80) | (df['cgpa'] < 5.11)]\nStep-6: Trimming of Outliers\n\nnew_df = df[(df['cgpa'] < 8.80) & (df['cgpa'] > 5.11)]\nnew_df\nStep-7: Capping on Outliers\n\nupper_limit = df['cgpa'].mean() + 3*df['cgpa'].std()\nlower_limit = df['cgpa'].mean() - 3*df['cgpa'].std()\nStep-8: Now, apply the Capping\n\ndf['cgpa'] = np.where(\n    df['cgpa']>upper_limit,\n    upper_limit,\n    np.where(\n        df['cgpa']<lower_limit,\n        lower_limit,\n        df['cgpa']\n    )\n)\nStep-9: Now see the statistics using “Describe” Function\n\ndf['cgpa'].describe()\nOutput:\n\ncount    1000.000000\nmean        6.961499\nstd         0.612688\nmin         5.113546\n25%         6.550000\n50%         6.960000\n75%         7.370000\nmax         8.808934\nName: cgpa, dtype: float64\nThis completes our Z-score based technique!\n\n \n\n👉 IQR based filtering :\n\nUsed when our data distribution is skewed.\n\nStep-1: Import necessary dependencies\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nStep-2: Read and Load the Dataset\n\ndf = pd.read_csv('placement.csv')\ndf.head()\nStep-3: Plot the distribution plot for the features\n\nplt.figure(figsize=(16,5))\nplt.subplot(1,2,1)\nsns.distplot(df['cgpa'])\nplt.subplot(1,2,2)\nsns.distplot(df['placement_exam_marks'])\nplt.show()\nStep-4: Form a Box-plot for the skewed feature\n\nsns.boxplot(df['placement_exam_marks'])\nDetect and remove outliers boxplot\n\nStep-5: Finding the IQR\n\npercentile25 = df['placement_exam_marks'].quantile(0.25)\npercentile75 = df['placement_exam_marks'].quantile(0.75)\nStep-6: Finding upper and lower limit\n\nupper_limit = percentile75 + 1.5 * iqr\nlower_limit = percentile25 - 1.5 * iqr\nStep-7: Finding Outliers\n\ndf[df['placement_exam_marks'] > upper_limit]\ndf[df['placement_exam_marks'] < lower_limit]\nStep-8: Trimming\n\nnew_df = df[df['placement_exam_marks'] < upper_limit]\nnew_df.shape\nStep-9: Compare the plots after trimming\n\nplt.figure(figsize=(16,8))\nplt.subplot(2,2,1)\nsns.distplot(df['placement_exam_marks'])\nplt.subplot(2,2,2)\nsns.boxplot(df['placement_exam_marks'])\nplt.subplot(2,2,3)\nsns.distplot(new_df['placement_exam_marks'])\nplt.subplot(2,2,4)\nsns.boxplot(new_df['placement_exam_marks'])\nplt.show()\ncomparison post trimming Detect and remove outliers\n\nStep-10: Capping\n\nnew_df_cap = df.copy()\nnew_df_cap['placement_exam_marks'] = np.where(\n    new_df_cap['placement_exam_marks'] > upper_limit,\n    upper_limit,\n    np.where(\n        new_df_cap['placement_exam_marks'] < lower_limit,\n        lower_limit,\n        new_df_cap['placement_exam_marks']\n    )\n)\nStep-11: Compare the plots after capping\n\nplt.figure(figsize=(16,8))\nplt.subplot(2,2,1)\nsns.distplot(df['placement_exam_marks'])\nplt.subplot(2,2,2)\nsns.boxplot(df['placement_exam_marks'])\nplt.subplot(2,2,3)\nsns.distplot(new_df_cap['placement_exam_marks'])\nplt.subplot(2,2,4)\nsns.boxplot(new_df_cap['placement_exam_marks'])\nplt.show()\ncomparison post capping\n\nThis completes our IQR based technique!\n\n \n\n👉 Percentile :\n\n– This technique works by setting a particular threshold value, which decides based on our problem statement.\n\n– While we remove the outliers using capping, then that particular method is known as Winsorization.\n\n– Here we always maintain symmetry on both sides means if remove 1% from the right then in the left we also drop by 1%.\n\nStep-1: Import necessary dependencies\n\nimport numpy as np\nimport pandas as pd\nStep-2: Read and Load the dataset\n\ndf = pd.read_csv('weight-height.csv')\ndf.sample(5)\ndata height\n\nStep-3: Plot the distribution plot of “height” feature\n\nsns.distplot(df['Height'])\nStep-4: Plot the box-plot of “height” feature\n\nsns.boxplot(df['Height'])\nboxplot height\n\nStep-5: Finding upper and lower limit\n\nupper_limit = df['Height'].quantile(0.99)\nlower_limit = df['Height'].quantile(0.01)\nStep-7: Apply trimming\n\nnew_df = df[(df['Height'] <= 74.78) & (df['Height'] >= 58.13)]\nStep-8: Compare the distribution and box-plot after trimming\n\nsns.distplot(new_df['Height'])\nsns.boxplot(new_df['Height'])\nDetect and remove outliers trriming boxplot\n\n👉 Winsorization :\n\nStep-9: Apply Capping(Winsorization)\n\ndf['Height'] = np.where(df['Height'] >= upper_limit,\n        upper_limit,\n        np.where(df['Height'] <= lower_limit,\n        lower_limit,\n        df['Height']))\nStep-10: Compare the distribution and box-plot after capping\n\nsns.distplot(df['Height'])\nsns.boxplot(df['Height'])\nboxplot post capping Detect and remove outliers\n\nThis completes our percentile-based technique!"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a939daee-b7ec-4088-b902-dcbea4f0da33","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## 2. Feature Selection\nNow that the dataset is clean, it is time to prepare a feature subspace containing features relevant to the model. The first step toward preparing this subspace is to use basic logic and reasoning to pick the relevant features. Then, you can use popular feature selection methods in ML: intrinsic methods, wrapper methods, or filter methods, depending on your dataset characteristics.\n\n## 3. Feature Engineering\nYou will often find variables in your dataset that can be converted into useful features using encoding methods. For example, the color of a flower can be used to distinguish it from others. But, as color is usually a string type of variable, it cannot be directly fed to a machine learning model. Using feature engineering techniques in such cases can prove to be useful. If you explore enough projects in data science, you will find feature engineering methods are commonly used for categorical feature variables."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f7bd1db2-b2d5-46f2-a0a9-f3085c7c4bea","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## III - Modeling"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b2668463-cb78-40d1-a6f1-0d73890b50eb","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["This will be developed in the next modules"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1b183d10-a448-43b4-baa6-31b44614c2b6","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## IV - Evaluation"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b5e2cf7a-7d4e-45dd-a0a3-2a73eb54f5b6","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["This will be developed in the next modules"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c5885604-b6f2-45b4-89f8-36d5101c120b","inputWidgets":{},"title":""}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"2_DS_workflow_FE","dashboards":[],"notebookMetadata":{},"language":"python","widgets":{},"notebookOrigID":1059930657351226}},"nbformat":4,"nbformat_minor":0}
