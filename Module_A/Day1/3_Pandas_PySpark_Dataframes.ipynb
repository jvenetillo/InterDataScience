{"cells":[{"cell_type":"markdown","source":["## From Pandas to PySpark \n\nSome parts of this material were taken from [Databricks Academy](https://customer-academy.databricks.com/) and from this [Source](https://medium.com/@bhanusree.balisetty/from-pandas-to-pyspark-e7188c8276e)."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c57d2e3f-94d0-4a7d-9020-cccc2733e87b","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["#### We love Pandas!  \n\nLearning programming with Pandas is like getting started with the “Hello World” program in the world of data science.  \nPandas is a widely used, intuitive, easy to learn Python library. It deals with Dataframes which store data in tabular format with rows and columns (spreadsheet format). Pandas loads all the data into the memory of the machine (Single Node) for faster execution.  \n\n<br>\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/Grosser_Panda.JPG/1280px-Grosser_Panda.JPG\" width=\"512\" height=\"384\" />\n\n#### Why Pyspark then?\n\nWhile Pandas stays one of the widely used libraries in dealing with tabular format data especially in Data Science, it does not fully support **parallelization**. Pyspark is a Python API for Spark. It has been released to support the collaboration between Python and Spark environments.\n\nPyspark with its *cluster computing* processes the data in a distributed manner by running the code on multiple nodes, leading to decreased execution times. With data being created exponentially every day, Data Scientists now have huge datasets to deal with, which is where distributed computing comes in. But what is Spark?"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"96d7498a-d8ff-4fa4-9f53-e7f4718e9a67","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["#### Apache Spark\n\nA single computer usually has the memory and computational power to perform calculations on data sets up to the size of a few gigabytes or less. Data sets larger than that either can't fit into the memory of a single computer or take an unacceptably long time for a single computer to process. For these types of \"big data\" use cases, we need a system that can split a large data set into smaller subsets &mdash; often referred to as **partitions** &mdash; and then distribute the processing of these data partitions across a number of computers.\n\n[Apache Spark](https://spark.apache.org/) is an open-source data processing engine that manages distributed processing of large data sets.\n\nFor example, let's say that we have a large data set and we want to calculate various statistics for some of its numeric columns. With Apache Spark, our program only needs to specify the data set to read and the statistics that we want calculated. We can then run the program on a set of computers that have been configured to serve as an Apache Spark **cluster**. When we run it, Spark automatically:\n\n* determines how to divide the data set into partitions,\n* assigns those partitions to the various computers of the cluster with instructions for calculating per-partition statistics, and\n* finally collects those per-partitions statistics and calculates the final results we requested.\n\nSpark was created originally as a research project at the University of California Berkeley. In 2013, the project was donated to the Apache Software Foundation. That same year the creators of Spark founded Databricks.\n\nDatabricks, in general, uses Apache Spark as the computation engine for the platform. Databricks provides simple management tools for running Spark clusters composed of cloud-provided virtual machines to process the data you have in cloud object storage and other systems."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b34cb771-f139-44ee-9576-dec73d573bde","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["-sandbox\n<img src=\"https://files.training.databricks.com/images/sparkcluster.png\" style=\"width:600px;height:250px;\">\n\nSo, if you want to move from a single node to multiple nodes and adapt to distributed cluster computing, this notebook will help in converting Pandas code to Pyspark code.  \nIt presents some of the commonly used Pandas Dataframe transformations and some miscellaneous operations along with the corresponding Pyspark syntax."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ebec7614-3647-4d3b-ba9b-2ef280e21cdd","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Let's start by importing the necessary packages:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3b407f0a-d246-4a85-9279-15a2e5a643be","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import os\nimport pandas as pd\nfrom datetime import timedelta\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7e99bc0d-f84c-474d-ac34-5fa99cd22135","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["In Databricks notebooks, the SparkSession is created for you, stored in a variable called spark. That is why the line below is commented:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"19b9776d-e24a-4f87-8260-a657b7236ce3","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# spark = SparkSession.builder.appName('spark_session').getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"151547a1-3f97-4fc2-b477-148d99e67345","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["![](https://miro.medium.com/max/1280/1*aJSwrFLDlDbf9axjJ7gXHw.jpeg)\n\nThe SparkSession class is the single entry point to all functionality in Spark using the DataFrame API.  \nIt provides a way to interact with various spark’s functionality in order to programmatically create PySpark RDD, DataFrame with a lesser number of constructs. Instead of having a spark context, hive context, SQL context, now all of it is encapsulated in a Spark session.\nMore information [here](https://sparkbyexamples.com/pyspark/pyspark-what-is-sparksession/)."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"80fb6928-6af2-4a78-a3a3-aa39c148b099","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["spark"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"928c8255-8d4c-4d6e-bdaf-fafae184a0b7","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### In the next sections, we will compare Pandas and PySpark regarding the following tasks:  \n\n1. Creating Dataframes\n2. Creating new Columns\n3. Updating existing Column data\n4. Select and Filtering data\n5. Column Type Transformations\n6. Rename, Drop Columns\n7. Melt Dataframes\n8. Add Interval to a Timestamp Column (Timedelta)\n9. Additional Syntax"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"161d8f58-0a70-431b-93f4-baeef03bb00f","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["#### 1 - Creating Dataframes"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e51f9227-49b4-41e9-afc6-cce694648d37","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Let us take a look at how we create dataframes from scratch in Pandas as compared to Pyspark."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c157aa62-cce5-4145-8c41-21ba85f722ec","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# PANDAS\ndf1 = [['A1', 'B1', 2, '21-12-2021 10:30'], \n       ['A2', 'B2', 4, '21-12-2021 10:40'], \n       ['A3', 'B3', 5, '21-12-2021 11:00']] \n\ndf1 = pd.DataFrame(df1, columns = ['A', 'B', 'Value', 'Date_Column'])\ndf1.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d37f3c32-2ddd-436c-b915-2083411f5421","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# PYSPARK\ndf2 = spark.createDataFrame([('A1', 'B1', 2, '21-12-2021 10:30'),\n                            ('A2', 'B2', 4, '21-12-2021 10:40'),\n                            ('A3', 'B3', 5, '21-12-2021 11:00')],\n                            ['A', 'B', 'Value', 'Date_Column'])\ndf2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"697f5a36-e9ee-4c75-9f1b-47480f953087","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df2.show(n=3, truncate=False, vertical=True)  #print top 3 rows vertically"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1fd6b6cc-0c4d-4ae7-8447-7e9e2f3df0a7","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df2.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5dc15885-0e29-4725-a56a-af6ababa45a5","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 2 - Creating New Columns"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9a750b08-900b-4661-a39d-b789325be104","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Next, we compare how we create new columns. \nIf you are coming from Pandas, the Pyspark syntax might be less intuitive at first. \n\nThe key to almost all column manipulation in Pyspark is the `withColumn()` method."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e0fef369-2511-44df-99b7-7246f9e51322","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# PANDAS - New column with constant values\ndf1['C'] = 'New Constant'\ndf1.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bcd48f23-ad25-4a85-849a-bb683707d3ec","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# PYSPARK - New column with nonstant values\ndf2 = df2.withColumn(\"C\", F.lit('New Constant'))\ndf2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8b51d983-d966-4136-b102-7e8acfb8888a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# PANDAS - New Column using existing columns\ndf1['C'] = df1['A'] + df1['B']\ndf1.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"faa6a460-b431-4ee1-a79f-03d5326b2351","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# PYSPARK - New Column using existing columns\ndf2 = df2.withColumn(\"C\", F.concat(\"A\", \"B\"))\ndf2.show()\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"44090ee4-b1e4-41ec-8f30-763b47b11607","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### NOTE\n- ``lit()`` – used to create constant columns\n- ``concat()`` – concatenate columns of dataframe\n- ``withColumn()`` – creates a new column"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0fb9eaf5-d4da-47e9-a9a5-158bd9153d45","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["#### 3 - Updating Existing Column Data"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"900bd02e-6bcf-42d4-a414-fbea0f4bc9c5","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# PANDAS - Update Column data\ndf1['Value'] = df1['Value']**2\ndf1.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1ce693cf-0e1f-4a66-90d6-2d07c27cdc32","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# PYSPARK - Update Column data\ndf2 = df2.withColumn(\"Value\", F.col(\"Value\")**2)\ndf2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7d2ab109-ad26-4508-acaa-8cfb41ee7945","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 4 - Selecting and Filtering Data"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c50a551d-8c64-4661-ac87-82c3f2cfe2fb","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# PANDAS - Selecting Columns\nnew_df1 = df1[['B', 'C']]\nnew_df1.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9fdd9b54-007d-4e79-b4c6-be0831cc986b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# PYSPARK - Selecting Columns\nnew_df2 = df2.select(\"B\", \"C\")\nnew_df2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1f357444-66c2-4c0a-bd40-752f00b6861c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df1"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d4ef5fb3-2ff7-4b98-aaed-b2266fff6b48","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# PANDAS - Filtering rows based on condition\nnew_df1 = df1[df1['Value']<5]\nnew_df1.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e505840f-e5fa-41cc-9dbc-fc5d5658d157","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# PYSPARK - Filtering rows based on condition\nnew_df2 = df2.filter(df2.Value<5)\nnew_df2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1ad4e2d9-bed7-4b68-9356-1c09381b049a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 5 - Column Type Transformations"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5aa82433-fc0f-4bf9-9dc0-f244e0921892","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# PANDAS - Convert Column from String to DateTime format\ndf1['Date_Column'] =  pd.to_datetime(df1['Date_Column'], format='%d-%m-%Y %H:%M')\ndf1.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"502c2baa-7b65-45c1-96eb-8f0535cc56ce","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# PYSPARK - Convert Column from String to Timestamp format\ndf2 = df2.withColumn(\"Date_Column\", F.to_timestamp(\"Date_Column\", \"dd-MM-yyyy hh:mm\"))\ndf2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9907b6f9-5c4d-4455-a393-5acc4a506ca4","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 6 - Rename, Drop Columns"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2f9fdcae-0e91-426b-83ca-fb0d72b8e1f1","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# PANDAS - Rename Columns\ndf1 = df1.rename(columns={'A': 'Col_A', 'B': 'Col_B'})\ndf1.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"93458f91-09d8-4185-bf63-cb9e8e8a93e4","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# PYSPARK - Rename Columns\ndf2 = df2.withColumnRenamed(\"A\", \"Col_A\").withColumnRenamed(\"B\", \"Col_B\")\ndf2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6ae3dc0b-c23a-4db2-b4ef-a951eb40f8e9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# PANDAS - Drop Columns\ndf1 = df1.drop(['Col_A', 'Col_B'], axis=1)\ndf1.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3b711ba5-a661-4f55-a670-f8bc2378a497","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# PYSPARK - Drop Columns\ndf2 = df2.drop('A', 'B')\ndf2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c2747f32-ed38-431b-a73a-e3610c3e14e5","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 7 - Melt Dataframes"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7ca08f5c-5418-402d-841c-b8c97acea943","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# PANDAS\ndf3 = pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c'},\n                    'B': {0: 1, 1: 3, 2: 5},\n                    'C': {0: 2, 1: 4, 2: 6}})\n\npd.melt(df3, id_vars=['A'], value_vars=['B', 'C'])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ad25eed9-20ad-4f87-970f-ad297fcb9bbf","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# PYSPARK custom melt function\ndef melt(df, id_vars, value_vars, var_name=\"Variable\", value_name=\"Value\"):\n    _vars_and_vals = F.array(*(F.struct(F.lit(c).alias(var_name),\n                                        F.col(c).alias(value_name)) for c in value_vars))\n    _tmp = df.withColumn(\"_vars_and_vals\",\n                         F.explode(_vars_and_vals))\n    cols = id_vars + [F.col(\"_vars_and_vals\")[x].alias(x) for x in [var_name, value_name]]\n    return _tmp.select(*cols)\n\n\ndf4 = spark.createDataFrame([('a', 1, 2), ('b', 3, 4), ('c', 5, 6)], ['A', 'B', 'C'])\n\nmelt(df4, ['A'], ['B', 'C']).display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1e44d22c-cafc-4659-9485-920cc51a2dc3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 8 - Add Interval to a Timestamp Column (Timedelta)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f795c46e-6822-4028-97e5-61a2d7704afb","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# PANDAS - Add 'Interval' to 'Start_Time'\ndf5 = pd.DataFrame([['2021-01-10 10:10:00', '00:05'],\n                    ['2021-12-10, 05:30:00', '00:15'],\n                    ['2021-11-10 11:40:00', '00:20']], \n                   columns = ['Start_Time','Interval'])\n\ndf5['Start_Time'] = pd.to_datetime(df5['Start_Time'])\ndf5['End_Time'] = df5['Start_Time'] + pd.to_timedelta(pd.to_datetime(df5['Interval']).dt.strftime('%H:%M:%S'))\ndf5.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b974fa0e-f427-40a5-a5fa-727fe9904f37","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# PYSPARK - Add 'Interval' to 'Start_Time'\ndf6 = spark.createDataFrame([['2021-01-10 10:10:00', '00:05'], \n                            ['2021-12-10 05:30:00', '00:15'], \n                            ['2021-11-10 11:40:00', '00:20']], \n                            ['Start_Time', 'Interval'])\n\ndf6 = df6.withColumn(\"Start_Time\", F.to_timestamp(\"Start_Time\", \"yyyy-MM-dd hh:mm:ss\"))\ndf6 = df6.withColumn(\"End_Time\", (F.unix_timestamp(\"Start_Time\") + F.unix_timestamp(\"Interval\", \"HH:mm\")).cast('timestamp'))\ndf6.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c91f4fdb-9a73-479c-aacb-b790626aa0d8","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 9 - Converting from Spark to Pandas and vice versa"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1c2f86a4-e6a0-4e44-89ce-994ea2b820a9","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["We have two dataframes, one being a Pandas dataframe, the other being a Pyspark dataframe."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"79b50a34-14d4-4d7e-8817-ff1846082cbc","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["print(type(df5))\nprint(type(df6))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"88e685f7-c20f-45a3-ab3f-548819357ef1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We want to convert from Pandas to Spark and from Spark to Pandas. \n\nTo convert from Pandas to Spark, we use `createDataFrame()`."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b07431dd-dad6-4ee9-8fca-9449e3bb2bf5","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["sparkDF = spark.createDataFrame(df5) \nsparkDF.printSchema()\nsparkDF.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dfc59419-c172-4840-9a74-b85d1569e6eb","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["On the other hand, to convert from Spark to Pandas, we use the `toPandas()` method."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6d72cf56-5965-4a82-8e12-8fc537286e0e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["pandasDF = df6.toPandas()\npandasDF.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"12cb9a18-1ada-4afd-989a-bddddfd881a4","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 10 - Additional Syntax"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"532bc5d6-6ff5-461b-820a-efa7fc056991","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# PANDAS df\ndf7 = pd.DataFrame({'A': {0: 'a', 1: 'a', 2: 'c'},\n                    'B': {0: 1, 1: 1, 2: 5},\n                    'C': {0: 2, 1: 4, 2: 6}})\ndf7"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b5f38a39-7cd9-4035-b9a8-0cfbdad3953c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# PANDAS - Shape of dataframe\nprint(df7.shape)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ffc30877-604f-4c07-9259-f222e159fe63","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# PANDAS - Distinct values of a column\ndf7['A'].unique()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3a0a9794-c268-4c2b-8875-e95190b7bfb6","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# PANDAS - Group by columns - Calculate aggregate functions\ndf7.groupby(['A', 'B']).sum()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d877f84f-ca13-4628-8094-691b4edc647d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# PYSPARK df\ndf8 = spark.createDataFrame([('a', 1, 2), ('a', 1, 4), ('c', 5, 6)],\n                            ['A', 'B', 'C'])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5a7ed258-b1a9-4716-82a7-c4f25617294e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# PYSPARK - Shape of dataframe\nprint((df8.count(), len(df4.columns)))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c46a2526-670b-48a0-9d49-48dbf6a3e595","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# PYSPARK - Distinct values of a column\ndf8.select('A').distinct().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1cba0a25-8e9f-43c2-b3d9-6145e49d5025","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# PYSPARK - Group by columns - calculate aggregate functions\ndf8.groupBy(\"A\", \"B\").agg(F.sum(\"C\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"10d20ad7-9b59-4d52-acd1-a974e5c0b175","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"3_Pandas_PySpark_Dataframes","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1059930657350674}},"nbformat":4,"nbformat_minor":0}
