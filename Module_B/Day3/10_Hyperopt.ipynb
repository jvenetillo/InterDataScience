{"cells":[{"cell_type":"markdown","source":["# Hyperopt\n\nHyperopt is a Python library for \"serial and parallel optimization over awkward search spaces, which may include real-valued, discrete, and conditional dimensions\".\n\nIn the machine learning workflow, hyperopt can be used to distribute/parallelize the hyperparameter optimization process with more advanced optimization strategies than are available in other libraries.\n\nThere are two ways to scale hyperopt with Apache Spark:\n* Use single-machine hyperopt with a distributed training algorithm (e.g. MLlib)\n* Use distributed hyperopt with single-machine training algorithms (e.g. scikit-learn) with the SparkTrials class. \n\nIn this lesson, we will use single-machine hyperopt with MLlib, but in the lab, you will see how to use hyperopt to distribute the hyperparameter tuning of single node models. \n\nUnfortunately you can’t use hyperopt to distribute the hyperparameter optimization for distributed training algorithms at this time. However, you do still get the benefit of using more advanced hyperparameter search algorthims (random search, TPE, etc.) with Spark ML.\n\n\nResources:\n0. [Documentation](http://hyperopt.github.io/hyperopt/scaleout/spark/)\n0. [Hyperopt on Databricks](https://docs.databricks.com/applications/machine-learning/automl/hyperopt/index.html)\n0. [Hyperparameter Tuning with MLflow, Apache Spark MLlib and Hyperopt](https://databricks.com/blog/2019/06/07/hyperparameter-tuning-with-mlflow-apache-spark-mllib-and-hyperopt.html)\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you:<br>\n - Use hyperopt to find the optimal parameters for an MLlib model using TPE"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d042b71c-be6a-456b-aa6d-3bd290bcb74d","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Let's start by loading in our SF Airbnb Dataset."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"96c39a4b-bbf2-4b16-928a-8c3c1033263e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import os"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7bf54649-3b42-405a-9f45-082dd2d0d065","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["## Put your name here\nusername = \"renato\"\n\ndbutils.widgets.text(\"username\", username)\nspark.sql(f\"CREATE DATABASE IF NOT EXISTS dsacademy_embedded_wave3_{username}\")\nspark.sql(f\"USE dsacademy_embedded_wave3_{username}\")\nspark.conf.set(\"spark.sql.shuffle.partitions\", 40)\n\nspark.sql(\"SET spark.databricks.delta.formatCheck.enabled = false\")\nspark.sql(\"SET spark.databricks.delta.properties.defaults.autoOptimize.optimizeWrite = true\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"58780bff-dda4-4c2a-8765-b1b4d8ef2ab4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[4]: DataFrame[key: string, value: string]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[4]: DataFrame[key: string, value: string]"]}}],"execution_count":0},{"cell_type":"code","source":["deltaPath = os.path.join(\"/\", \"tmp\", username)    #If we were writing to the root folder and not to the DBFS\nif not os.path.exists(deltaPath):\n    os.mkdir(deltaPath)\n    \nprint(deltaPath)\n\nairbnbDF = spark.read.format(\"delta\").load(deltaPath)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3d4e5127-9f2a-4aab-8451-bea9835bd71d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"/tmp/renato\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["/tmp/renato\n"]}}],"execution_count":0},{"cell_type":"code","source":["(trainDF, testDF) = airbnbDF.randomSplit([.8, .2], seed=42)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ca57bfa4-dd6b-4a4a-8816-08c3c179b855","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We will then create our random forest pipeline and regression evaluator."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e735980a-6c16-4465-b753-17b230365f9f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\ncategoricalCols = [field for (field, dataType) in trainDF.dtypes if dataType == \"string\"]\nindexOutputCols = [x + \"Index\" for x in categoricalCols]\n\nstringIndexer = StringIndexer(inputCols=categoricalCols, outputCols=indexOutputCols, handleInvalid=\"skip\")\n\nnumericCols = [field for (field, dataType) in trainDF.dtypes if ((dataType == \"double\") & (field != \"price\"))]\nassemblerInputs = indexOutputCols + numericCols\nvecAssembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n\nrf = RandomForestRegressor(labelCol=\"price\", maxBins=40, seed=42)\n\npipeline = Pipeline(stages=[stringIndexer, vecAssembler, rf])\n\nregressionEvaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"price\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"564c2972-0291-4c56-9b7e-ca7bde6240d9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Next, we get to the hyperopt-specific part of the workflow.\n\nFirst, we define our **objective function**. The objective function has two primary requirements:\n\n1. An **input** `params` including hyperparameter values to use when training the model\n2. An **output** containing a loss metric on which to optimize\n\nIn this case, we are specifying values of `max_depth` and `num_trees` and returning the RMSE as our loss metric.\n\nWe are reconstructing our pipeline for the `RandomForestRegressor` to use the specified hyperparameter values."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"78e2f9bd-5195-4fa1-b4bc-09e311d3bc6c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nimport mlflow\n\ndef objective_function(params):    \n  # set the hyperparameters that we want to tune\n  max_depth = params[\"max_depth\"]\n  num_trees = params[\"num_trees\"]\n\n  # create a grid with our hyperparameters\n  grid = (ParamGridBuilder()\n    .addGrid(rf.maxDepth, [max_depth])\n    .addGrid(rf.numTrees, [num_trees])\n    .build())\n\n  # cross validate the set of hyperparameters\n  cv = CrossValidator(estimator=pipeline, estimatorParamMaps=grid, evaluator=regressionEvaluator, numFolds=3)\n  cvModel = cv.fit(trainDF)\n\n  # get our average RMSE across all three folds\n  rmse = cvModel.avgMetrics[0]\n\n  return {\"loss\": rmse, \"status\": STATUS_OK}"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"50b7c742-952a-46e6-83b6-f0528ec84327","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Next, we define our search space. \n\nThis is similar to the parameter grid in a grid search process. However, we are only specifying the range of values rather than the individual, specific values to be tested. It's up to hyperopt's optimization algorithm to choose the actual values.\n\nSee the [documentation](https://github.com/hyperopt/hyperopt/wiki/FMin) for helpful tips on defining your search space."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9486cbd1-82e2-4106-b2e1-857aab9fd841","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from hyperopt import hp\n\nsearch_space = {\n  \"max_depth\": hp.randint(\"max_depth\", 2, 5),\n  \"num_trees\": hp.randint(\"num_trees\", 10, 100)\n}"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"920edd49-1cd1-451e-8130-83e79ca1937d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["`fmin()` generates new hyperparameter configurations to use for your `objective_function`. It will evaluate 4 models in total, using the information from the previous models to make a more informative decision for the the next hyperparameter to try. \n\nHyperopt allows for parallel hyperparameter tuning using either random search or Tree of Parzen Estimators (TPE). Note that in the cell below, we are importing `tpe`. According to the [documentation](http://hyperopt.github.io/hyperopt/scaleout/spark/), TPE is an adaptive algorithm that \n\n> iteratively explores the hyperparameter space. Each new hyperparameter setting tested will be chosen based on previous results. \n\nHence, `tpe.suggest` is a Bayesian method.\n\nMLflow also integrates with Hyperopt, so you can track the results of all the models you’ve trained and their results as part of your hyperparameter tuning. Notice you can track the MLflow experiment in this notebook, but you can also specify an external experiment."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"59713a74-16e7-4c70-ab92-ffb85eb4ad32","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from hyperopt import fmin, tpe, STATUS_OK, Trials\nimport numpy as np\n\n# Creating a parent run\nwith mlflow.start_run():\n  num_evals = 4\n  trials = Trials()\n  best_hyperparam = fmin(fn=objective_function, \n                         space=search_space,\n                         algo=tpe.suggest, \n                         max_evals=num_evals,\n                         trials=trials,\n                         rstate=np.random.RandomState(42)\n                        )\n  \n  # get optimal hyperparameter values\n  best_max_depth = best_hyperparam[\"max_depth\"]\n  best_num_trees = best_hyperparam[\"num_trees\"]\n  \n  # change RF to use optimal hyperparameter values (this is a stateful method)\n  rf.setMaxDepth(best_max_depth)\n  rf.setNumTrees(best_num_trees)\n  \n  # train pipeline on entire training data - this will use the updated RF values\n  pipelineModel = pipeline.fit(trainDF)\n  \n  # evaluate final model on test data\n  predDF = pipelineModel.transform(testDF)\n  rmse = regressionEvaluator.evaluate(predDF)\n  \n  # Log param and metric for the final model\n  mlflow.log_param(\"max_depth\", best_max_depth)\n  mlflow.log_param(\"numTrees\", best_num_trees)\n  mlflow.log_metric(\"rmse\", rmse)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e5e501b0-3531-490c-9db6-41baec919110","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\r  0%|          | 0/4 [00:00<?, ?trial/s, best loss=?]\r  0%|          | 0/4 [00:00<?, ?trial/s, best loss=?]\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\r  0%|          | 0/4 [00:00<?, ?trial/s, best loss=?]\r  0%|          | 0/4 [00:00<?, ?trial/s, best loss=?]\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n\u001B[0;32m<command-74902913649367>\u001B[0m in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      6\u001B[0m   \u001B[0mnum_evals\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m4\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m   \u001B[0mtrials\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTrials\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 8\u001B[0;31m   best_hyperparam = fmin(fn=objective_function, \n\u001B[0m\u001B[1;32m      9\u001B[0m                          \u001B[0mspace\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msearch_space\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m                          \u001B[0malgo\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtpe\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msuggest\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/.python_edge_libs/hyperopt/fmin.py\u001B[0m in \u001B[0;36mfmin\u001B[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001B[0m\n\u001B[1;32m    563\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    564\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mallow_trials_fmin\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrials\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"fmin\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 565\u001B[0;31m         return trials.fmin(\n\u001B[0m\u001B[1;32m    566\u001B[0m             \u001B[0mfn\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    567\u001B[0m             \u001B[0mspace\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/.python_edge_libs/hyperopt/base.py\u001B[0m in \u001B[0;36mfmin\u001B[0;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001B[0m\n\u001B[1;32m    669\u001B[0m         \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mfmin\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mfmin\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    670\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 671\u001B[0;31m         return fmin(\n\u001B[0m\u001B[1;32m    672\u001B[0m             \u001B[0mfn\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    673\u001B[0m             \u001B[0mspace\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/.python_edge_libs/hyperopt/fmin.py\u001B[0m in \u001B[0;36mfmin\u001B[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001B[0m\n\u001B[1;32m    609\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    610\u001B[0m     \u001B[0;31m# next line is where the fmin is actually executed\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 611\u001B[0;31m     \u001B[0mrval\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexhaust\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    612\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    613\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mreturn_argmin\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/.python_edge_libs/hyperopt/fmin.py\u001B[0m in \u001B[0;36mexhaust\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    387\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mexhaust\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    388\u001B[0m         \u001B[0mn_done\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrials\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 389\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmax_evals\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mn_done\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mblock_until_done\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0masynchronous\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    390\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrials\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrefresh\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    391\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/.python_edge_libs/hyperopt/fmin.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(self, N, block_until_done)\u001B[0m\n\u001B[1;32m    297\u001B[0m                     \u001B[0;31m# processes orchestration\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    298\u001B[0m                     new_trials = algo(\n\u001B[0;32m--> 299\u001B[0;31m                         \u001B[0mnew_ids\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdomain\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrials\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrstate\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mintegers\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m2\u001B[0m \u001B[0;34m**\u001B[0m \u001B[0;36m31\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    300\u001B[0m                     )\n\u001B[1;32m    301\u001B[0m                     \u001B[0;32massert\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnew_ids\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnew_trials\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAttributeError\u001B[0m: 'numpy.random.mtrand.RandomState' object has no attribute 'integers'","errorSummary":"<span class='ansi-red-fg'>AttributeError</span>: 'numpy.random.mtrand.RandomState' object has no attribute 'integers'","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n\u001B[0;32m<command-74902913649367>\u001B[0m in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      6\u001B[0m   \u001B[0mnum_evals\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m4\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m   \u001B[0mtrials\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTrials\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 8\u001B[0;31m   best_hyperparam = fmin(fn=objective_function, \n\u001B[0m\u001B[1;32m      9\u001B[0m                          \u001B[0mspace\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msearch_space\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m                          \u001B[0malgo\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtpe\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msuggest\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/.python_edge_libs/hyperopt/fmin.py\u001B[0m in \u001B[0;36mfmin\u001B[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001B[0m\n\u001B[1;32m    563\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    564\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mallow_trials_fmin\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrials\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"fmin\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 565\u001B[0;31m         return trials.fmin(\n\u001B[0m\u001B[1;32m    566\u001B[0m             \u001B[0mfn\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    567\u001B[0m             \u001B[0mspace\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/.python_edge_libs/hyperopt/base.py\u001B[0m in \u001B[0;36mfmin\u001B[0;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001B[0m\n\u001B[1;32m    669\u001B[0m         \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mfmin\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mfmin\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    670\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 671\u001B[0;31m         return fmin(\n\u001B[0m\u001B[1;32m    672\u001B[0m             \u001B[0mfn\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    673\u001B[0m             \u001B[0mspace\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/.python_edge_libs/hyperopt/fmin.py\u001B[0m in \u001B[0;36mfmin\u001B[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001B[0m\n\u001B[1;32m    609\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    610\u001B[0m     \u001B[0;31m# next line is where the fmin is actually executed\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 611\u001B[0;31m     \u001B[0mrval\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexhaust\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    612\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    613\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mreturn_argmin\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/.python_edge_libs/hyperopt/fmin.py\u001B[0m in \u001B[0;36mexhaust\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    387\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mexhaust\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    388\u001B[0m         \u001B[0mn_done\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrials\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 389\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmax_evals\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mn_done\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mblock_until_done\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0masynchronous\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    390\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrials\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrefresh\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    391\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/.python_edge_libs/hyperopt/fmin.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(self, N, block_until_done)\u001B[0m\n\u001B[1;32m    297\u001B[0m                     \u001B[0;31m# processes orchestration\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    298\u001B[0m                     new_trials = algo(\n\u001B[0;32m--> 299\u001B[0;31m                         \u001B[0mnew_ids\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdomain\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrials\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrstate\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mintegers\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m2\u001B[0m \u001B[0;34m**\u001B[0m \u001B[0;36m31\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    300\u001B[0m                     )\n\u001B[1;32m    301\u001B[0m                     \u001B[0;32massert\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnew_ids\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnew_trials\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAttributeError\u001B[0m: 'numpy.random.mtrand.RandomState' object has no attribute 'integers'"]}}],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2020 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4dca7ca9-8d82-4d3c-8deb-ad9bbddfb9a4","inputWidgets":{},"title":""}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"10_Hyperopt","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{"username":{"nuid":"ebd20970-0e23-416c-8ecc-685b8a7a1c24","currentValue":"renato","widgetInfo":{"widgetType":"text","name":"username","defaultValue":"renato","label":null,"options":{"widgetType":"text","validationRegex":null}}}},"notebookOrigID":74902913649275}},"nbformat":4,"nbformat_minor":0}
